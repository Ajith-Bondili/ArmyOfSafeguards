# Army of Safeguards

A modular collection of AI safety safeguards for detecting various types of harmful or problematic content.

## ğŸ—ï¸ Project Structure

```
ArmyOfSafeguards/
â”œâ”€â”€ factuality/              # Factuality checking safeguard
â”‚   â”œâ”€â”€ safeguard_factuality.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ toxicity/                # Toxicity detection (coming soon)
â”œâ”€â”€ sexual/                  # Sexual content detection (coming soon)
â”œâ”€â”€ jailbreak/               # Jailbreak attempt detection (coming soon)
â”œâ”€â”€ aggregator/              # Unified interface for all safeguards
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tests/                   # Test scripts and results
â”‚   â”œâ”€â”€ test_factuality.py
â”‚   â”œâ”€â”€ quick_test.py
â”‚   â””â”€â”€ TEST_RESULTS.md
â”œâ”€â”€ requirements.txt         # Shared dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Setup

```bash
# Clone the repository
git clone https://github.com/SohamNagi/ArmyOfSafeguards.git
cd ArmyOfSafeguards

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Aggregator

Evaluate text using all available safeguards:

```bash
python aggregator/aggregator.py "Your text to evaluate here"
```

### 3. Use Individual Safeguards

Each safeguard can be used independently:

```bash
# Factuality check
python factuality/safeguard_factuality.py "The Earth is flat."
```

## ğŸ“¦ Available Safeguards

### âœ… Factuality Safeguard
- **Status**: Complete
- **Model**: `ajith-bondili/deberta-v3-factuality-small`
- **Purpose**: Detects factually incorrect or misleading statements
- **Developer**: Ajith
- [View Documentation](factuality/README.md)

### ğŸš§ Coming Soon
- **Toxicity Detection** (Soham)
- **Sexual Content Detection** (Jian)
- **Jailbreak Detection** (Tommy)

## ğŸ”§ Usage

### Python API

```python
# Use the aggregator for comprehensive evaluation
from aggregator.aggregator import evaluate_text

result = evaluate_text("Your text here", threshold=0.7)
print(f"Is Safe: {result['is_safe']}")
print(f"Flags: {result['flags']}")

# Or use individual safeguards
from factuality.safeguard_factuality import predict

result = predict("The sky is blue.")
print(f"Label: {result['label']}, Confidence: {result['confidence']:.2%}")
```

### Command Line

```bash
# Run all safeguards
python aggregator/aggregator.py "Text to check"

# Run specific safeguard
python factuality/safeguard_factuality.py "Text to check"
```

## ğŸ§ª Testing

```bash
# Run all tests
python tests/test_factuality.py

# Quick sanity check
python tests/quick_test.py
```

## ğŸ¤ Contributing

Each team member maintains their own safeguard module:

1. Create your safeguard in its own directory (e.g., `toxicity/`)
2. Implement `predict()` function that returns `{"label": str, "confidence": float}`
3. Add your safeguard to the aggregator
4. Include tests and documentation

## ğŸ“ Requirements

- Python 3.9+
- PyTorch
- Transformers
- See `requirements.txt` for full list

## ğŸ“„ License

[Add license information]

## ğŸ‘¥ Team

- **Ajith**: Factuality Safeguard
- **Soham**: Toxicity Safeguard
- **Jian**: Sexual Content Safeguard
- **Tommy**: Jailbreak Safeguard

