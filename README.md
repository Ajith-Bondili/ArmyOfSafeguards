# Army of Safeguards

A modular collection of AI safety safeguards for detecting various types of harmful or problematic content.

## ğŸ—ï¸ Project Structure

```
ArmyOfSafeguards/
â”œâ”€â”€ factuality/              # Factuality checking safeguard
â”‚   â”œâ”€â”€ safeguard_factuality.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ toxicity/                # Toxicity detection (coming soon)
â”œâ”€â”€ sexual/                  # Sexual content detection (coming soon)
â”œâ”€â”€ jailbreak/               # Jailbreak attempt detection (coming soon)
â”œâ”€â”€ aggregator/              # Unified interface for all safeguards
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tests/                   # Test scripts and results
â”‚   â”œâ”€â”€ test_factuality.py
â”‚   â”œâ”€â”€ quick_test.py
â”‚   â””â”€â”€ TEST_RESULTS.md
â”œâ”€â”€ requirements.txt         # Shared dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Setup

```bash
# Clone the repository
git clone https://github.com/SohamNagi/ArmyOfSafeguards.git
cd ArmyOfSafeguards

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Aggregator

Evaluate text using all available safeguards:

```bash
python aggregator/aggregator.py "Your text to evaluate here"
```

### 3. Use Individual Safeguards

Each safeguard can be used independently:

```bash
# Factuality check
python factuality/safeguard_factuality.py "The Earth is flat."
```

## ğŸ“¦ Available Safeguards

### âœ… Factuality Safeguard
- **Status**: Complete
- **Model**: `ajith-bondili/deberta-v3-factuality-small`
- **Purpose**: Detects factually incorrect or misleading statements
- **Developer**: Ajith
- [View Documentation](factuality/README.md)

### ğŸš§ Coming Soon
- **Toxicity Detection** (Soham)
- **Sexual Content Detection** (Jian)
- **Jailbreak Detection** (Tommy)

## ğŸ”§ Usage

### Python API

```python
# Use the aggregator for comprehensive evaluation
from aggregator.aggregator import evaluate_text

result = evaluate_text("Your text here", threshold=0.7)
print(f"Is Safe: {result['is_safe']}")
print(f"Flags: {result['flags']}")

# Or use individual safeguards
from factuality.safeguard_factuality import predict

result = predict("The sky is blue.")
print(f"Label: {result['label']}, Confidence: {result['confidence']:.2%}")
```

### Command Line

```bash
# Run all safeguards
python aggregator/aggregator.py "Text to check"

# Run specific safeguard
python factuality/safeguard_factuality.py "Text to check"
```

## ğŸ§ª Testing & Evaluation

Each safeguard has its own test suite in its directory:

```bash
# Factuality tests
python factuality/tests/test_factuality.py

# Quick sanity check
python factuality/tests/quick_test.py

# Benchmark (prediction distribution)
python factuality/tests/benchmark_factuality.py

# Full evaluation (accuracy, precision, recall, F1)
python factuality/tests/evaluate_factuality.py
```

### Evaluation Results

**Factuality Safeguard Performance**:

âš ï¸ **Note**: Model trained on TruthfulQA & FEVER - use OOD datasets for true generalization.

**Out-of-Distribution (True Generalization)**:
| Dataset | Accuracy | F1-Score | Domain |
|---------|----------|----------|--------|
| VitaminC | 54.00% | 36.11% | General claims |
| Climate-FEVER | 81.00% | - | Climate-specific |
| LIAR | 81.00% | - | Political statements |

**Training Data (Sanity Check)**:
| Dataset | Accuracy | F1-Score |
|---------|----------|----------|
| FEVER | 84.00% | 78.38% |
| TruthfulQA | 75.00% | - |

### Benchmark Datasets

The factuality safeguard can be evaluated on:
- **TruthfulQA** - LLM factuality benchmark
- **FEVER** - Wikipedia claim verification  
- **SciFact** - Scientific factuality
- **VitaminC** - Contradiction-aware claims
- **Climate-FEVER** - Climate misinformation

See `factuality/tests/` for benchmark and evaluation scripts.

## ğŸ¤ Contributing

Each team member maintains their own safeguard module:

1. Create your safeguard in its own directory (e.g., `toxicity/`)
2. Implement `predict()` function that returns `{"label": str, "confidence": float}`
3. Add your safeguard to the aggregator
4. Include tests and documentation

## ğŸ“ Requirements

- Python 3.9+
- PyTorch
- Transformers
- See `requirements.txt` for full list

## ğŸ“„ License

[Add license information]

## ğŸ‘¥ Team

- **Ajith**: Factuality Safeguard
- **Soham**: Toxicity Safeguard
- **Jian**: Sexual Content Safeguard
- **Tommy**: Jailbreak Safeguard

