"""
Benchmark stub for factuality safeguard.

This script outlines how to evaluate the DeBERTa-v3 factuality model
on multiple public factuality datasets.

Phase 1: Framework setup (this file)
Phase 2: Full evaluation with metrics
"""
import sys
from pathlib import Path

# Add parent directory to path so we can import the safeguard
sys.path.insert(0, str(Path(__file__).parent.parent))

from datasets import load_dataset
from safeguard_factuality import predict
from tqdm import tqdm


# Benchmark datasets for factuality evaluation
BENCHMARKS = {
    "TruthfulQA": {
        "hf_id": "truthful_qa",
        "config": "generation",
        "split": "validation",
        "text_field": "question",
        "label_field": "category"
    },
    "FEVER": {
        "hf_id": "fever",
        "config": "v1.0",
        "split": "paper_test",
        "text_field": "claim",
        "label_field": "label"
    },
    "SciFact": {
        "hf_id": "allenai/scifact",
        "config": "claims",
        "split": "test",
        "text_field": "claim",
        "label_field": "label"
    },
    "VitaminC": {
        "hf_id": "tals/vitaminc",
        "config": None,
        "split": "test",
        "text_field": "claim",
        "label_field": "label"
    },
    "Climate-FEVER": {
        "hf_id": "climate_fever",
        "config": None,
        "split": "test",
        "text_field": "claim",
        "label_field": "claim_label"
    }
}


def run_benchmark(name: str, config: dict, limit: int = 100, verbose: bool = True):
    """
    Run the factuality safeguard on a sample of the dataset.
    
    Args:
        name: Name of the benchmark dataset
        config: Configuration dictionary with dataset details
        limit: Maximum number of examples to evaluate
        verbose: Whether to print progress
        
    Returns:
        Dictionary with evaluation results
    """
    if verbose:
        print(f"\n{'='*60}")
        print(f"Benchmark: {name}")
        print(f"{'='*60}")
    
    try:
        # Load dataset
        hf_id = config["hf_id"]
        ds_config = config.get("config")
        split = config.get("split", "test")
        
        if ds_config:
            ds = load_dataset(hf_id, ds_config, split=split, trust_remote_code=True)
        else:
            ds = load_dataset(hf_id, split=split, trust_remote_code=True)
        
        # Sample subset
        subset_size = min(limit, len(ds))
        subset = ds.select(range(subset_size))
        
        if verbose:
            print(f"Loaded {subset_size} examples from {name}")
            print(f"Evaluating with factuality safeguard...\n")
        
        # Run predictions
        predictions = []
        labels = []
        confidences = []
        
        for ex in tqdm(subset, disable=not verbose):
            # Extract text
            text_field = config.get("text_field", "claim")
            text = ex.get(text_field, str(ex))
            
            # Get prediction
            result = predict(text)
            predictions.append(result["label"])
            confidences.append(result["confidence"])
            
            # Extract ground truth (if available)
            label_field = config.get("label_field")
            if label_field and label_field in ex:
                labels.append(ex[label_field])
        
        # Calculate basic statistics
        avg_confidence = sum(confidences) / len(confidences)
        label_0_count = predictions.count("LABEL_0")
        label_1_count = predictions.count("LABEL_1")
        
        results = {
            "dataset": name,
            "total_examples": subset_size,
            "predictions": {
                "LABEL_0 (factual)": label_0_count,
                "LABEL_1 (non-factual)": label_1_count
            },
            "average_confidence": avg_confidence,
            "factual_rate": label_0_count / subset_size
        }
        
        if verbose:
            print(f"\nResults:")
            print(f"  Total examples: {results['total_examples']}")
            print(f"  LABEL_0 (factual): {label_0_count} ({label_0_count/subset_size:.1%})")
            print(f"  LABEL_1 (non-factual): {label_1_count} ({label_1_count/subset_size:.1%})")
            print(f"  Average confidence: {avg_confidence:.2%}")
        
        return results
        
    except Exception as e:
        if verbose:
            print(f"⚠️  Could not run {name}: {e}")
        return {"dataset": name, "error": str(e)}


def run_all_benchmarks(limit: int = 100):
    """
    Run all benchmark evaluations.
    
    Args:
        limit: Maximum number of examples per dataset
        
    Returns:
        List of results dictionaries
    """
    print("="*60)
    print("FACTUALITY SAFEGUARD BENCHMARK SUITE")
    print("="*60)
    print(f"\nEvaluating on {len(BENCHMARKS)} datasets (limit: {limit} examples each)")
    
    all_results = []
    
    for name, config in BENCHMARKS.items():
        result = run_benchmark(name, config, limit=limit, verbose=True)
        all_results.append(result)
    
    # Summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    
    successful = [r for r in all_results if "error" not in r]
    failed = [r for r in all_results if "error" in r]
    
    print(f"\nSuccessful: {len(successful)}/{len(BENCHMARKS)}")
    
    if successful:
        print("\nFactual rates by dataset:")
        for result in successful:
            print(f"  {result['dataset']}: {result['factual_rate']:.1%}")
    
    if failed:
        print(f"\nFailed: {len(failed)}")
        for result in failed:
            print(f"  {result['dataset']}: {result['error']}")
    
    return all_results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Benchmark factuality safeguard on standard datasets"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="Maximum examples per dataset (default: 100)"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(BENCHMARKS.keys()),
        help="Run single dataset only"
    )
    
    args = parser.parse_args()
    
    if args.dataset:
        # Run single dataset
        config = BENCHMARKS[args.dataset]
        run_benchmark(args.dataset, config, limit=args.limit)
    else:
        # Run all datasets
        run_all_benchmarks(limit=args.limit)

