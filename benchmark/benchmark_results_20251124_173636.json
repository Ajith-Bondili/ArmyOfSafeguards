{
  "timestamp": "2025-11-24T17:36:36.452765",
  "system": "Army of Safeguards (Aggregator)",
  "threshold": 0.5,
  "limit_per_benchmark": 100,
  "results": [
    {
      "benchmark": "HarmBench",
      "timestamp": "2025-11-24T17:35:50.508959",
      "threshold": 0.5,
      "total_examples": 100,
      "accuracy": 0.77,
      "precision": 1.0,
      "recall": 0.77,
      "f1_score": 0.8700564971751412,
      "average_confidence": 0.837664926449458,
      "confusion_matrix": {
        "safe_safe": 0,
        "safe_unsafe": 0,
        "unsafe_safe": 23,
        "unsafe_unsafe": 77
      },
      "ground_truth_distribution": {
        "safe": 0,
        "unsafe": 100
      },
      "prediction_distribution": {
        "safe": 23,
        "unsafe": 77
      },
      "per_class_metrics": {
        "safe": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "support": 0
        },
        "unsafe": {
          "precision": 1.0,
          "recall": 0.77,
          "f1_score": 0.8700564971751412,
          "support": 100
        }
      }
    },
    {
      "benchmark": "JailbreakBench",
      "timestamp": "2025-11-24T17:36:12.408057",
      "threshold": 0.5,
      "total_examples": 100,
      "accuracy": 0.57,
      "precision": 0.5373134328358209,
      "recall": 0.75,
      "f1_score": 0.6260869565217392,
      "average_confidence": 0.8290053234497706,
      "confusion_matrix": {
        "safe_safe": 21,
        "safe_unsafe": 31,
        "unsafe_safe": 12,
        "unsafe_unsafe": 36
      },
      "ground_truth_distribution": {
        "safe": 52,
        "unsafe": 48
      },
      "prediction_distribution": {
        "safe": 33,
        "unsafe": 67
      },
      "per_class_metrics": {
        "safe": {
          "precision": 0.6363636363636364,
          "recall": 0.40384615384615385,
          "f1_score": 0.49411764705882355,
          "support": 52
        },
        "unsafe": {
          "precision": 0.5373134328358209,
          "recall": 0.75,
          "f1_score": 0.6260869565217392,
          "support": 48
        }
      }
    },
    {
      "benchmark": "WildGuardMix",
      "timestamp": "2025-11-24T17:36:36.452391",
      "threshold": 0.5,
      "total_examples": 99,
      "accuracy": 0.46464646464646464,
      "precision": 0.42105263157894735,
      "recall": 0.7804878048780488,
      "f1_score": 0.5470085470085471,
      "average_confidence": 0.8330984069442104,
      "confusion_matrix": {
        "safe_safe": 14,
        "safe_unsafe": 44,
        "unsafe_safe": 9,
        "unsafe_unsafe": 32
      },
      "ground_truth_distribution": {
        "safe": 58,
        "unsafe": 41
      },
      "prediction_distribution": {
        "safe": 23,
        "unsafe": 76
      },
      "per_class_metrics": {
        "safe": {
          "precision": 0.6086956521739131,
          "recall": 0.2413793103448276,
          "f1_score": 0.345679012345679,
          "support": 58
        },
        "unsafe": {
          "precision": 0.42105263157894735,
          "recall": 0.7804878048780488,
          "f1_score": 0.5470085470085471,
          "support": 41
        }
      }
    }
  ],
  "summary": {
    "avg_accuracy": 0.6015488215488215,
    "avg_precision": 0.6527886881382561,
    "avg_recall": 0.766829268292683,
    "avg_f1_score": 0.6810506669018093
  }
}